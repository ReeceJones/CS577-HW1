{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import decomposition\n",
    "import altair as alt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "from main import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "all_data = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "stops = set(stopwords.words('english'))\n",
    "def clean(x):\n",
    "    a = pos_tag(word_tokenize(x.lower()))\n",
    "    wnpos = lambda e: ('a' if e[0].lower() == 'j' else e[0].lower()) if e[0].lower() in ['n', 'r', 'v'] else 'n'\n",
    "    b = [wnl.lemmatize(y[0], wnpos(y[1])) for y in a]\n",
    "    return [y for y in b if y not in stops]\n",
    "\n",
    "sentences = all_data.text.apply(clean).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['feel', 'awful', 'job', 'get', 'position', 'succeed', 'happen'], ['im', 'alone', 'feel', 'awful'], ['ive', 'probably', 'mention', 'really', 'feel', 'proud', 'actually', 'keep', 'new', 'year', 'resolution', 'monthly', 'weekly', 'goal']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_document(list_of_list_of_words):\n",
    "   for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "\n",
    "def expand_vector(x):\n",
    "   d = {'id': x.id}\n",
    "   for i in range(len(x.text)):\n",
    "      d[f'vec_{i}'] = x.text[i]\n",
    "   return pd.Series(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train doc2vec model using provided data\n",
    "tweet_train_data = list(tagged_document(sentences))\n",
    "tweet_model = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=30)\n",
    "tweet_model.build_vocab(tweet_train_data)\n",
    "tweet_model.train(tweet_train_data, total_examples=tweet_model.corpus_count, epochs=tweet_model.epochs)\n",
    "tweet_embeddings = pd.DataFrame(columns=['id', 'text'])\n",
    "for idx, row in all_data.iterrows():\n",
    "    tweet_embeddings.loc[len(tweet_embeddings)] = pd.Series({'id': row.id, 'text': list(tweet_model.infer_vector(clean(row.text)))})\n",
    "\n",
    "tweet_embeddings.apply(expand_vector, axis=1).to_csv('tweet_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tain doc2vec model using text8\n",
    "# text8_train_data = list(tagged_document([d for d in api.load('wiki-english-20171001')]))\n",
    "# text8_model = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=30)\n",
    "# text8_model.build_vocab(text8_train_data)\n",
    "# text8_model.train(text8_train_data, total_examples=text8_model.corpus_count, epochs=text8_model.epochs)\n",
    "# text8_embeddings = pd.DataFrame(columns=['id', 'text'])\n",
    "# for idx, row in all_data.iterrows():\n",
    "#     text8_embeddings.loc[len(text8_embeddings)] = pd.Series({'id': row.id, 'text': list(text8_model.infer_vector(clean(row.text)))})\n",
    "\n",
    "# text8_embeddings.apply(expand_vector, axis=1).to_csv('text8_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive document model using glove-200\n",
    "naive_glove_embeddings = pd.DataFrame(columns=['id', 'text'])\n",
    "model = api.load('glove-twitter-200')\n",
    "for idx, row in all_data.iterrows():\n",
    "    c = clean(row.text)\n",
    "    naive_glove_embeddings.loc[len(naive_glove_embeddings)] = pd.Series({'id': row.id, 'text': list(np.sum([model[w] if w in model else np.zeros(200) for w in c], axis=0) / len(c))})\n",
    "\n",
    "naive_glove_embeddings.drop_duplicates(subset=['id'], keep='first').apply(expand_vector, axis=1).to_csv('naive_glove_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      _befriend_0  _befriend_1  _befriend_2  _befriend_3  _befriend_4  \\\n",
      "0             0.0          0.0          0.0          0.0          0.0   \n",
      "1             0.0          0.0          0.0          0.0          0.0   \n",
      "2             0.0          0.0          0.0          0.0          0.0   \n",
      "3             0.0          0.0          0.0          0.0          0.0   \n",
      "4             0.0          0.0          0.0          0.0          0.0   \n",
      "...           ...          ...          ...          ...          ...   \n",
      "1195          0.0          0.0          0.0          0.0          0.0   \n",
      "1196          0.0          0.0          0.0          0.0          0.0   \n",
      "1197          0.0          0.0          0.0          0.0          0.0   \n",
      "1198          0.0          0.0          0.0          0.0          0.0   \n",
      "1199          0.0          0.0          0.0          0.0          0.0   \n",
      "\n",
      "      _befriend_5  _befriend_6  _befriend_7  _befriend_8  _befriend_9  ...  \\\n",
      "0             0.0          0.0          0.0          0.0          0.0  ...   \n",
      "1             0.0          0.0          0.0          0.0          0.0  ...   \n",
      "2             0.0          0.0          0.0          0.0          0.0  ...   \n",
      "3             0.0          0.0          0.0          0.0          0.0  ...   \n",
      "4             0.0          0.0          0.0          0.0          0.0  ...   \n",
      "...           ...          ...          ...          ...          ...  ...   \n",
      "1195          0.0          0.0          0.0          0.0          0.0  ...   \n",
      "1196          0.0          0.0          0.0          0.0          0.0  ...   \n",
      "1197          0.0          0.0          0.0          0.0          0.0  ...   \n",
      "1198          0.0          0.0          0.0          0.0          0.0  ...   \n",
      "1199          0.0          0.0          0.0          0.0          0.0  ...   \n",
      "\n",
      "      _phase_40  _phase_41  _phase_42  _phase_43  _phase_44  _phase_45  \\\n",
      "0           0.0        0.0        0.0        0.0        0.0        0.0   \n",
      "1           0.0        0.0        0.0        0.0        0.0        0.0   \n",
      "2           0.0        0.0        0.0        0.0        0.0        0.0   \n",
      "3           0.0        0.0        0.0        0.0        0.0        0.0   \n",
      "4           0.0        0.0        0.0        0.0        0.0        0.0   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "1195        0.0        0.0        0.0        0.0        0.0        0.0   \n",
      "1196        0.0        0.0        0.0        0.0        0.0        0.0   \n",
      "1197        0.0        0.0        0.0        0.0        0.0        0.0   \n",
      "1198        0.0        0.0        0.0        0.0        0.0        0.0   \n",
      "1199        0.0        0.0        0.0        0.0        0.0        0.0   \n",
      "\n",
      "      _phase_46  _phase_47  _phase_48  _phase_49  \n",
      "0           0.0        0.0        0.0        0.0  \n",
      "1           0.0        0.0        0.0        0.0  \n",
      "2           0.0        0.0        0.0        0.0  \n",
      "3           0.0        0.0        0.0        0.0  \n",
      "4           0.0        0.0        0.0        0.0  \n",
      "...         ...        ...        ...        ...  \n",
      "1195        0.0        0.0        0.0        0.0  \n",
      "1196        0.0        0.0        0.0        0.0  \n",
      "1197        0.0        0.0        0.0        0.0  \n",
      "1198        0.0        0.0        0.0        0.0  \n",
      "1199        0.0        0.0        0.0        0.0  \n",
      "\n",
      "[1200 rows x 150950 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reece/Documents/Purdue/CS577/HW1/venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/reece/Documents/Purdue/CS577/HW1/venv/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:595: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.explained_variance_ratio_ = self.explained_variance_ / total_var.sum()\n",
      "/Users/reece/Documents/Purdue/CS577/HW1/venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/reece/Documents/Purdue/CS577/HW1/venv/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:595: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.explained_variance_ratio_ = self.explained_variance_ / total_var.sum()\n",
      "/Users/reece/Documents/Purdue/CS577/HW1/venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/reece/Documents/Purdue/CS577/HW1/venv/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:595: RuntimeWarning: invalid value encountered in true_divide\n",
      "  self.explained_variance_ratio_ = self.explained_variance_ / total_var.sum()\n",
      "/Users/reece/Documents/Purdue/CS577/HW1/venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/reece/Documents/Purdue/CS577/HW1/venv/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:501: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "/Users/reece/Documents/Purdue/CS577/HW1/venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/reece/Documents/Purdue/CS577/HW1/venv/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:501: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n",
      "/Users/reece/Documents/Purdue/CS577/HW1/venv/lib/python3.9/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/reece/Documents/Purdue/CS577/HW1/venv/lib/python3.9/site-packages/sklearn/decomposition/_pca.py:501: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio_ = explained_variance_ / total_var\n"
     ]
    }
   ],
   "source": [
    "# PCA document model use glove-50\n",
    "pca_glove_embeddings = pd.DataFrame(columns=['id', 'text'])\n",
    "model = api.load('glove-twitter-50')\n",
    "# for idx, row in all_data.iterrows():\n",
    "    # pca_glove_embeddings.loc[len(pca_glove_embeddings)] = pd.Series({'id': row.id, 'text': np.nan_to_num(np.concatenate([model[w] if w in model else np.zeros(50) for w in clean(row.text)]).flat)})\n",
    "df, _, __= preprocess(train, remove_stopwords=True, lemmatize=True)\n",
    "del df['id']\n",
    "del df['emotions']\n",
    "\n",
    "def blow_up(x):\n",
    "    d = dict()\n",
    "    for i,y in x.iteritems():\n",
    "        v = model[i] if i in model else np.zeros(50)\n",
    "        for j in range(len(v)):\n",
    "            d[f'{i}_{j}'] = v[j]\n",
    "    return pd.Series(d)\n",
    "\n",
    "pca_glove_embeddings = df.apply(blow_up, axis=1)\n",
    "\n",
    "# pca_glove_embeddings = pca_glove_embeddings.apply(expand_vector, axis=1)\n",
    "print(pca_glove_embeddings)\n",
    "vector_matrix = np.matrix(pca_glove_embeddings.loc[:, ~pca_glove_embeddings.columns.isin(['id'])])\n",
    "\n",
    "pca = decomposition.PCA(1200)\n",
    "transformed_matrix = pca.fit_transform(vector_matrix)\n",
    "\n",
    "df = pd.DataFrame(transformed_matrix)\n",
    "df['id'] = train.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv('pca_glove_embeddings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2fe0cf8c5c9198125d7f764f92867b61fea50ee858b9c810b6cee1236a4ab216"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
